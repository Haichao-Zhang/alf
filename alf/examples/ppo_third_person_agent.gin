#include 'ppo.gin'

# environment config
import alf.algorithms.ppo_loss
import alf.algorithms.ppo_algorithm
import alf.trainers.off_policy_trainer
import alf.algorithms.actor_critic_algorithm

import alf.algorithms.multi_agent_algorithm
import alf.environments.suite_socialbot

# expert phase
#TeacherActorCriticAlgorithm.teacher_training_phase=True
create_environment.env_name='SocialBot-Third-Person-Teacher-v0'

# imitation phase
#TeacherActorCriticAlgorithm.teacher_training_phase=False
#create_environment.env_name='SocialBot-Third-Person-Agent-v0'
#value/MultiActorValueNetwork.input_tensor_spec=%learner_observation_spec

create_environment.env_load_fn=@suite_socialbot.load
create_environment.num_parallel_environments=1

# algorithm config
# ActorCriticAlgorithm.gradient_clipping=0.5
# ActorCriticAlgorithm.clip_by_global_norm=True

PPOLoss.entropy_regularization=0.0
PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss
PPOLoss.check_numerics=True

learner/get_sub_observation_spec.index=0
teacher/get_sub_observation_spec.index=1
learner_observation_spec=@learner/get_sub_observation_spec()
teacher_observation_spec=@teacher/get_sub_observation_spec()

learner/get_sub_action_spec.index=0
teacher/get_sub_action_spec.index=1
learner_action_spec=@learner/get_sub_action_spec()
teacher_action_spec=@teacher/get_sub_action_spec()

action_spec=@get_action_spec()

NormalProjectionNetwork.init_means_output_factor=1e-10
NormalProjectionNetwork.std_bias_initializer_value=5.0
NormalProjectionNetwork.std_transform=@tf.math.exp

learner/ActorDistributionNetwork.input_tensor_spec=%learner_observation_spec
learner/ActorDistributionNetwork.output_tensor_spec=%learner_action_spec
learner/ActorDistributionNetwork.fc_layer_params=(100, 50, 25)
learner/ActorDistributionNetwork.activation_fn=@tf.nn.softsign
learner/ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork

teacher/ActorDistributionNetwork.input_tensor_spec=%teacher_observation_spec
teacher/ActorDistributionNetwork.output_tensor_spec=%teacher_action_spec
teacher/ActorDistributionNetwork.fc_layer_params=(100, 50, 25)
teacher/ActorDistributionNetwork.activation_fn=@tf.nn.softsign
teacher/ActorDistributionNetwork.continuous_projection_net=@NormalProjectionNetwork

learner/ValueNetwork.input_tensor_spec=%learner_observation_spec
learner/ValueNetwork.fc_layer_params=(100, 50, 25)
learner/ValueNetwork.activation_fn=@tf.nn.softsign

teacher/ValueNetwork.input_tensor_spec=%teacher_observation_spec
teacher/ValueNetwork.fc_layer_params=(100, 50, 25)
teacher/ValueNetwork.activation_fn=@tf.nn.softsign

learner/Adam.learning_rate=0e-4
teacher/Adam.learning_rate=2e-4


learner/Agent.action_spec=%learner_action_spec
learner/ActorCriticAlgorithm.loss_class=@PPOLoss
learner/ActorCriticAlgorithm.actor_network=@learner/ActorDistributionNetwork()
learner/ActorCriticAlgorithm.value_network=@learner/ValueNetwork()
learner/Agent.optimizer=@learner/Adam()

teacher/Agent.action_spec=%teacher_action_spec
teacher/ActorCriticAlgorithm.loss_class=@PPOLoss
teacher/ActorCriticAlgorithm.actor_network=@teacher/ActorDistributionNetwork()
teacher/ActorCriticAlgorithm.value_network=@teacher/ValueNetwork()
teacher/Agent.optimizer=@learner/Adam()


MultiAgentAlgorithm.algos=[@learner/Agent, @teacher/Agent]
MultiAgentAlgorithm.action_spec=%action_spec

# training config
# multi-agent policy trainer
TrainerConfig.trainer=@sync_off_policy_trainer
# can append another algorithm or provide a algorithm list
TrainerConfig.algorithm_ctor=@MultiAgentAlgorithm
TrainerConfig.mini_batch_length=1
TrainerConfig.unroll_length=100
TrainerConfig.mini_batch_size=6000
TrainerConfig.num_iterations=100000
TrainerConfig.num_updates_per_train_step=25
TrainerConfig.eval_interval=1000
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summary_interval=10

TFUniformReplayBuffer.max_length=2048