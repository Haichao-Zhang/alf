# naf for half cheetah
import alf.algorithms.agent
import alf.algorithms.naf_algorithm
import pybullet_envs

# environment config
NUM_PARALLEL_ENVIRONMENTS=1
env_name='HalfCheetahBulletEnv-v0'

create_environment.env_name=%env_name
create_environment.num_parallel_environments=%NUM_PARALLEL_ENVIRONMENTS

max_len=200
load.max_episode_steps=%max_len
#load.gym_env_wrappers=(@ContinuousActionNormalization, )


# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()

actor/ActorNetwork.input_tensor_spec=%observation_spec
actor/ActorNetwork.action_spec=%action_spec
actor/ActorNetwork.fc_layer_params=(100,100)
actor/Adam.lr=1e-4


critic/NafCriticNetwork.input_tensor_spec=(%observation_spec, %action_spec)
critic/NafCriticNetwork.observation_fc_layer_params=(100,100)
critic/NafCriticNetwork.use_last_kernel_initializer=True
critic/NafCriticNetwork.mu_fc_layer_params=(100, 100)
critic/NafCriticNetwork.v_fc_layer_params=(100, 100)
critic/NafCriticNetwork.l_fc_layer_params=(100, 100)
critic/NafCriticNetwork.cov_mode="full"
critic/Adam.lr=1e-3

# NafAlgorithm.actor_network=@actor/ActorNetwork()
# NafAlgorithm.actor_optimizer=@actor/Adam()

NafAlgorithm.critic_network=@critic/NafCriticNetwork()
NafAlgorithm.critic_optimizer=@critic/Adam()

#RLAlgorithm.gradient_clipping=1.0
#RLAlgorithm.clip_by_global_norm=True
NafAlgorithm.target_update_period=5
NafAlgorithm.target_update_tau=0.05
OneStepTDLoss.td_error_loss_fn=@losses.element_wise_huber_loss






# training config
TrainerConfig.initial_collect_steps=1000
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=1
TrainerConfig.mini_batch_size=128
TrainerConfig.sample_batch_num=1
TrainerConfig.num_updates_per_train_step=1
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
# TrainerConfig.algorithm_ctor=@NafAlgorithm
TrainerConfig.num_iterations=200000
TrainerConfig.num_checkpoints=5
TrainerConfig.evaluate=0
TrainerConfig.eval_interval=1000
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=1
TrainerConfig.summary_interval=100
TrainerConfig.replay_buffer_length=1000000
TrainerConfig.algorithm_ctor=@Agent
Agent.rl_algorithm_cls=@NafAlgorithm
TrainerConfig.random_seed=1586987476
