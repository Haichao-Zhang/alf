# naf for pendulum

# default ddpg config

import alf.algorithms.naf_algorithm

# environment config
NUM_PARALLEL_ENVIRONMENTS=1
create_environment.env_name="Pendulum-v0"
create_environment.num_parallel_environments=%NUM_PARALLEL_ENVIRONMENTS

max_len=200
load.max_episode_steps=%max_len
#load.gym_env_wrappers=(@ContinuousActionNormalization, )


# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()


critic/NafCriticNetwork.input_tensor_spec=(%observation_spec, %action_spec)
critic/NafCriticNetwork.observation_fc_layer_params=(100,100)
critic/NafCriticNetwork.use_last_kernel_initializer=False
# critic/NafCriticNetwork.mu_fc_layer_params=(50, 10)
# critic/NafCriticNetwork.v_fc_layer_params=(50, 10)
# critic/NafCriticNetwork.l_fc_layer_params=(50, 10)
critic/Adam.lr=1e-3

NafAlgorithm.critic_network=@critic/NafCriticNetwork()
NafAlgorithm.critic_optimizer=@critic/Adam()

NafAlgorithm.target_update_period=1
NafAlgorithm.target_update_tau=0.1
OneStepTDLoss.td_error_loss_fn=@losses.element_wise_huber_loss


# training config
TrainerConfig.initial_collect_steps=1000
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=1
TrainerConfig.mini_batch_size=64
TrainerConfig.num_updates_per_train_step=5
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
TrainerConfig.algorithm_ctor=@NafAlgorithm
TrainerConfig.num_iterations=10000
TrainerConfig.num_checkpoints=5
TrainerConfig.evaluate=0
TrainerConfig.eval_interval=100
TrainerConfig.debug_summaries=1
TrainerConfig.summarize_grads_and_vars=1
TrainerConfig.summary_interval=100
TrainerConfig.replay_buffer_length=100000
TrainerConfig.random_seed=1586713914

